{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11673938,"sourceType":"datasetVersion","datasetId":7326570},{"sourceId":374050,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":309279,"modelId":329661}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install necessary package\n!pip install -U --no-cache-dir bitsandbytes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T08:11:04.872155Z","iopub.execute_input":"2025-05-05T08:11:04.872325Z","iopub.status.idle":"2025-05-05T08:12:05.518776Z","shell.execute_reply.started":"2025-05-05T08:11:04.872309Z","shell.execute_reply":"2025-05-05T08:12:05.518087Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m271.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m269.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m310.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m275.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m288.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m238.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m315.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m318.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Imports\nimport json\nfrom typing import List\nfrom pydantic import BaseModel\nfrom datasets import Dataset, load_from_disk\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    pipeline\n)\nfrom peft import LoraConfig, get_peft_model, PeftModel\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T08:12:05.520558Z","iopub.execute_input":"2025-05-05T08:12:05.520788Z","iopub.status.idle":"2025-05-05T08:12:32.574354Z","shell.execute_reply.started":"2025-05-05T08:12:05.520767Z","shell.execute_reply":"2025-05-05T08:12:32.573743Z"}},"outputs":[{"name":"stderr","text":"2025-05-05 08:12:20.317170: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746432740.501064      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746432740.552090      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Pydantics\n","metadata":{}},{"cell_type":"code","source":"# --- Pydantic Model for Input Validation ---\nclass EventRecord(BaseModel):\n    event_text: str\n    output: dict\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T08:12:32.575031Z","iopub.execute_input":"2025-05-05T08:12:32.575492Z","iopub.status.idle":"2025-05-05T08:12:32.651102Z","shell.execute_reply.started":"2025-05-05T08:12:32.575473Z","shell.execute_reply":"2025-05-05T08:12:32.650418Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Data Pre-processing ","metadata":{}},{"cell_type":"code","source":"# --- Load and Prepare Data ---\ndef load_jsonl_dataset(file_path: str) -> Dataset:\n    \"\"\"Load a .jsonl dataset into a HuggingFace Dataset\"\"\"\n    with open(file_path, \"r\") as f:\n        samples = [EventRecord(**json.loads(line)).dict() for line in f]\n    return Dataset.from_list(samples)\n\n# --- Tokenization ---\ndef tokenize(example: dict) -> dict:\n    \"\"\"Tokenizes input/output pair and creates labels for training.\"\"\"\n    input_text = example[\"event_text\"]\n    output_text = json.dumps(example[\"output\"], ensure_ascii=False)\n    full_text = f\"{input_text}\\n{output_text}\"\n\n    # Tokenize combined text\n    tokenized = tokenizer(\n        full_text,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512,\n    )\n\n    # Create labels (mask input part)\n    input_ids = tokenizer(input_text, truncation=True, max_length=512)[\"input_ids\"]\n    input_len = len(input_ids)\n    labels = tokenized[\"input_ids\"][:]\n    labels[:input_len] = [-100] * input_len\n    tokenized[\"labels\"] = labels\n\n    return tokenized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T08:12:32.651746Z","iopub.execute_input":"2025-05-05T08:12:32.652011Z","iopub.status.idle":"2025-05-05T08:12:32.657415Z","shell.execute_reply.started":"2025-05-05T08:12:32.651984Z","shell.execute_reply":"2025-05-05T08:12:32.656838Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# --- Load and Split Dataset ---\ndata_path = \"/kaggle/input/keyword-extraction-calender-dataset/event_text_mapping.jsonl\"\ndataset = load_jsonl_dataset(data_path)\ndataset = dataset.train_test_split(test_size=0.1)\n\n# --- Load Tokenizer ---\nmodel_id = \"HuggingFaceTB/SmolLM-360M\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n\n# --- Tokenize and Save Dataset ---\ntokenized_dataset = dataset.map(tokenize)\nsave_path = \"./tokenized_event_dataset\"\ntokenized_dataset.save_to_disk(save_path)\n\n# --- Load Tokenized Dataset ---\ntokenized_dataset = load_from_disk(save_path)\ntrain_dataset = tokenized_dataset[\"train\"]\neval_dataset = tokenized_dataset[\"test\"]\n\n# --- Load Model ---\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T08:12:32.658105Z","iopub.execute_input":"2025-05-05T08:12:32.658311Z","iopub.status.idle":"2025-05-05T08:12:41.007846Z","shell.execute_reply.started":"2025-05-05T08:12:32.658297Z","shell.execute_reply":"2025-05-05T08:12:41.007350Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3067122703.py:5: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  samples = [EventRecord(**json.loads(line)).dict() for line in f]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d19f277f03324000940550ce7d583e14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"730a9cc35efb46739cb60fcc939f3d07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa01e3598cc643868e5f3acf5f39a220"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79dd4207a1164f1fb848ef66d5ab791a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3222e5decb8b4920b708ab8fafa0179e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/712 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67dc13ff0047438c83e2554238544938"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/80 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8faf3e25d5c24e43b4b86da6ad64addf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/712 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d68747cbb0340e889ff92adc7d9d984"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/80 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f11f5e89ade41f59975763b08dad25f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89ab73affbe7455d97630b71d9894ab7"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.45G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66c3a865fd224fa38062fe9f55dc2975"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"276988fb5e8e4a2a852f676e1926ef65"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Configurations","metadata":{}},{"cell_type":"code","source":"# --- LoRA Configuration ---\ntarget_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"up_proj\", \"down_proj\"]\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=target_modules,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, peft_config)\n\n\n# --- Training Configuration ---\ntraining_args = TrainingArguments(\n    output_dir=\"./results_lora\",\n    num_train_epochs=10,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=8,\n    warmup_steps=100,\n    weight_decay=0.01,\n    logging_dir=\"./logs_lora\",\n    logging_steps=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    learning_rate=2e-5,\n    max_grad_norm=1.0,\n    lr_scheduler_type=\"cosine\",\n    gradient_checkpointing=False,\n    report_to=\"none\",\n    fp16=True\n)\n\n# --- Define Trainer ---\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T08:13:16.428580Z","iopub.execute_input":"2025-05-05T08:13:16.429324Z","iopub.status.idle":"2025-05-05T08:13:16.693299Z","shell.execute_reply.started":"2025-05-05T08:13:16.429295Z","shell.execute_reply":"2025-05-05T08:13:16.692669Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/628594744.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# --- Train and Save ---\ntrainer.train()\ntrainer.save_model(\"./fine_tuned_lora_model\")\nprint(\"✅ LoRA fine-tuning complete! Model saved to ./fine_tuned_lora_model\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# --- Inference Function ---\ndef inference(input_text: str):\n    \"\"\"Run inference on the fine-tuned model.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n\n    base_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n    # model = PeftModel.from_pretrained(base_model, \"./fine_tuned_lora_model\")\n    model = PeftModel.from_pretrained(base_model, \"/kaggle/input/finetuned_calender_extract/transformers/default/1\")\n    model.eval()\n\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n\n    with torch.no_grad():\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=128,\n            do_sample=False,\n            temperature=0.0,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n\n    output = tokenizer.decode(output_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\n    match = re.search(r'\\{.*?\\}', output, re.DOTALL)\n    first_json = json.loads(match.group()) if match else None\n\n    return first_json\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T08:27:18.794050Z","iopub.execute_input":"2025-05-05T08:27:18.794346Z","iopub.status.idle":"2025-05-05T08:27:18.800319Z","shell.execute_reply.started":"2025-05-05T08:27:18.794325Z","shell.execute_reply":"2025-05-05T08:27:18.799720Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# --- Run Sample Inference ---\ninference(\"Meeting on 05 - December - 2023, 3pm, lasting 1 hour, with Sarah and James on Google Meet.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T08:28:00.264468Z","iopub.execute_input":"2025-05-05T08:28:00.265031Z","iopub.status.idle":"2025-05-05T08:28:10.416507Z","shell.execute_reply.started":"2025-05-05T08:28:00.265002Z","shell.execute_reply":"2025-05-05T08:28:10.415707Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"{'action': 'Meeting',\n 'attendees': ['Sarah', 'James'],\n 'date': '05/12/2023',\n 'duration': '1 hour',\n 'location': 'Google Meet',\n 'notes': None,\n 'recurrence': None,\n 'time': '3:00 PM'}"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"# Download model by zipping","metadata":{}},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\n\ndef download_folder_as_zip(folder_path = \"/kaggle/working/fine_tuned_lora_model\", zip_name=\"finetuned_way2.zip\"):\n    \"\"\"\n    Zips the given folder and creates a download link for it in a Kaggle notebook.\n    \n    Args:\n        folder_path (str): Path to the folder to be zipped.\n        zip_name (str): Name of the zip file to create.\n    \"\"\"\n    # Zip the folder\n    shutil.make_archive(zip_name.replace(\".zip\", \"\"), 'zip', folder_path)\n    \n    # Provide a download link\n    display(FileLink(zip_name))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T08:12:41.591222Z","iopub.status.idle":"2025-05-05T08:12:41.591531Z","shell.execute_reply.started":"2025-05-05T08:12:41.591371Z","shell.execute_reply":"2025-05-05T08:12:41.591385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"download_folder_as_zip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T08:12:41.592786Z","iopub.status.idle":"2025-05-05T08:12:41.593518Z","shell.execute_reply.started":"2025-05-05T08:12:41.593385Z","shell.execute_reply":"2025-05-05T08:12:41.593401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}